{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1b02b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import needed packages\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import PIL \n",
    "from gtda.homology import CubicalPersistence\n",
    "\n",
    "from topologylayer.nn import LevelSetLayer2D, SumBarcodeLengths, PartialSumBarcodeLengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "310fd67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        \n",
    "#         self.pdfn = LevelSetLayer2D(size=(224,224),  sublevel=False)\n",
    "#         self.topfn = PartialSumBarcodeLengths(dim=1, skip=1) # penalize more than 1 hole\n",
    "#         self.topfn2 = SumBarcodeLengths(dim=0) # penalize more than 1 max\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc = nn.Linear(in_features= 2*5*128, out_features=128)\n",
    "        self.fc1 = nn.Linear(in_features= 128, out_features=num_classes)\n",
    "    def forward(self, input):\n",
    "#         output = self.pdfn(input)\n",
    "#         output = self.topfn(output)+self.topfn2(output)\n",
    "        \n",
    "#         print('shape before convolution ' + str(input.shape))\n",
    "        output = self.conv1(input)\n",
    "        output = self.relu1(output)\n",
    "#         print('shape after convolution ' + str(output.shape))\n",
    "        output = self.pool(output)\n",
    "#         print('shape after POOL ' + str(output.shape))\n",
    "        output = self.conv2(output)\n",
    "        output = self.relu2(output)\n",
    "#         print('shape after convolution ' + str(output.shape))\n",
    "        output = self.pool(output)\n",
    "#         print('shape after POOL ' + str(output.shape))\n",
    "        output = self.conv3(output)\n",
    "        output = self.relu3(output)\n",
    "#         print('shape after convolution ' + str(output.shape))\n",
    "        output = self.pool(output)\n",
    "#         print('shape after POOL ' + str(output.shape))\n",
    "        output = self.conv4(output)\n",
    "        \n",
    "#         print('shape after convolution ' + str(output.shape))\n",
    "        output = self.relu4(output)\n",
    "#         print('shape before linear  ' + str(output.shape))\n",
    "        output = self.pool(output)\n",
    "#         print('shape after POOL ' + str(output.shape))\n",
    "        output = output.view(output.size(0),-1)\n",
    "#         print('shape after linear ' + str(output.shape))\n",
    "\n",
    "        output = self.fc(output)\n",
    "#         print(output.shape)\n",
    "        output = self.fc1(output)\n",
    "#         print(output.shape)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a10bc0fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Images</th>\n",
       "      <th>labels</th>\n",
       "      <th>encoded_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No_Poacher_0_3749.jpg</td>\n",
       "      <td>Poacher with Gun</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No_Poacher_0_2206.jpg</td>\n",
       "      <td>Poacher with Gun</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No_Poacher_0_1980.jpg</td>\n",
       "      <td>Poacher with Gun</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No_Poacher_0_5938.jpg</td>\n",
       "      <td>Poacher with Gun</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No_Poacher_0_5743.jpg</td>\n",
       "      <td>Poacher with Gun</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Images            labels  encoded_labels\n",
       "0  No_Poacher_0_3749.jpg  Poacher with Gun               2\n",
       "1  No_Poacher_0_2206.jpg  Poacher with Gun               2\n",
       "2  No_Poacher_0_1980.jpg  Poacher with Gun               2\n",
       "3  No_Poacher_0_5938.jpg  Poacher with Gun               2\n",
       "4  No_Poacher_0_5743.jpg  Poacher with Gun               2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# #___LOAD DATA\n",
    "\n",
    "#___PATH\n",
    "BASE_PATH=\"/home/codepython/tensor_flow/Poacher_augmented_images/\"\n",
    "\n",
    "image=[]\n",
    "labels=[]\n",
    "for file in os.listdir(BASE_PATH):\n",
    "    if file=='No Poacher':\n",
    "        for c in os.listdir(os.path.join(BASE_PATH, file)):\n",
    "            if c!='annotations':\n",
    "                image.append(c)\n",
    "                labels.append('No Poacher')\n",
    "    if file=='Poacher with Gun':\n",
    "        for c in os.listdir(os.path.join(BASE_PATH, file)):\n",
    "            if c!='annotations':\n",
    "                image.append(c)\n",
    "                labels.append('Poacher with Gun')\n",
    "    if file=='Poacher with Arrow':\n",
    "        for c in os.listdir(os.path.join(BASE_PATH, file)):\n",
    "            if c!='annotations':\n",
    "                image.append(c)\n",
    "                labels.append('Poacher with Arrow')\n",
    "\n",
    "data = {'Images':image, 'labels':labels} \n",
    "data = pd.DataFrame(data) \n",
    "#data.head()\n",
    "\n",
    "\n",
    "lb = LabelEncoder()\n",
    "data['encoded_labels'] = lb.fit_transform(data['labels'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb1aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "        #transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5), (0.5))])\n",
    "\n",
    "#____POACHER___DATASET\n",
    "class Poacher_Dataset(Dataset):\n",
    "    def __init__(self, img_data,img_path,transform=None):\n",
    "        self.img_path = img_path\n",
    "        self.transform = transform\n",
    "        self.img_data = img_data\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_name = os.path.join(self.img_path,self.img_data.loc[index, 'labels'],\n",
    "                                self.img_data.loc[index, 'Images'])\n",
    "        image = Image.open(img_name)\n",
    "        image=np.array(image)\n",
    "        R, G, B = image[:,:,0], image[:,:,1], image[:,:,2]\n",
    "### Convert image to gray scale\n",
    "        image1=0.2989 * R + 0.5870 * G + 0.1140 * B\n",
    "### Extract topological imformation using Cubical Persistence\n",
    "###(Betti numbers 0-dimention and 1-dimention)        \n",
    "        image1=CubicalPersistence().fit_transform(image1)\n",
    "\n",
    "### Pading the cubical persistence image with zeros\n",
    "        \n",
    "        npad=((0,0),(224-image1.shape[1],0),(0,0))\n",
    "        image1=np.pad(image1, pad_width=npad, mode='constant', constant_values=0)\n",
    "\n",
    "        image=np.concatenate((image,image1),axis=1)\n",
    "\n",
    "        label = torch.tensor(self.img_data.loc[index, 'encoded_labels'])\n",
    "        if self.transform is not None:\n",
    "#             image=torch.from_numpy(image).long()\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "579336fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "validation_split = .3\n",
    "shuffle_dataset = True\n",
    "random_seed= 42\n",
    "\n",
    "dataset_size = len(data)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "\n",
    "\n",
    "#Define transformations for the training set, flip the images randomly, crop out and apply mean and std normalization\n",
    "train_transformations = transforms.Compose([\n",
    "     #transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(32,padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cd893cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create model, optimizer and loss function\n",
    "model = SimpleNet(num_classes=3)\n",
    "\n",
    "dataset = Poacher_Dataset(data,BASE_PATH,transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler)\n",
    "\n",
    "# model=SimpleNet()\n",
    "optimizer = Adam(model.parameters(), lr=0.001,weight_decay=0.0001)\n",
    "\n",
    "loss_fn= nn.CrossEntropyLoss()\n",
    "\n",
    "# tloss = TopLoss((50,50)) # topology penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4f4015d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1455845162.py, line 14)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_4681/1455845162.py\"\u001b[0;36m, line \u001b[0;32m14\u001b[0m\n\u001b[0;31m    lr = lr / 1000\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "#Create a learning rate adjustment function that divides the learning rate by 10 every 30 epochs\n",
    "def adjust_learning_rate(epoch):\n",
    "\n",
    "    lr = 0.001\n",
    "\n",
    "    if epoch > 180:\n",
    "        lr = lr / 1000000\n",
    "    elif epoch > 150:\n",
    "        lr = lr / 100000\n",
    "    elif epoch > 120:\n",
    "        lr = lr / 10000\n",
    "    elif epoch > 90:\n",
    "      \n",
    "    lr = lr / 1000\n",
    "    elif epoch > 60:\n",
    "        lr = lr / 100\n",
    "    elif epoch > 30:\n",
    "        lr = lr / 10\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fa31f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(model, optimizer, loss_fn, train_loader, test_loader, epochs=20, device='cpu'):\n",
    "\n",
    "    print('train() called: model=%s, opt=%s(lr=%f), epochs=%d, device=%s\\n' % \\\n",
    "          (type(model).__name__, type(optimizer).__name__,\n",
    "           optimizer.param_groups[0]['lr'], epochs, device))\n",
    "\n",
    "    history = {} # Collects per-epoch loss and acc like Keras' fit().\n",
    "    history['loss'] = []\n",
    "    history['val_loss'] = []\n",
    "    history['acc'] = []\n",
    "    history['val_acc'] = []\n",
    "\n",
    "    start_time_sec = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "\n",
    "        # --- TRAIN AND EVALUATE ON TRAINING SET -----------------------------\n",
    "        model.train()\n",
    "        train_loss         = 0.0\n",
    "        num_train_correct  = 0\n",
    "        num_train_examples = 0\n",
    "        confusion_matrixK = torch.zeros(3, 3)\n",
    "        for batch in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x    = batch[0].to(device)\n",
    "            \n",
    "            y    = batch[1].to(device)\n",
    "#             print(x.shape)\n",
    "#             print(y.shape)\n",
    "            yhat = model(x.float())\n",
    "#             print(yhat.shape)\n",
    "            loss = loss_fn(yhat, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss         += loss.data.item() * x.size(0)\n",
    "            num_train_correct  += (torch.max(yhat, 1)[1] == y).sum().item()\n",
    "            num_train_examples += x.shape[0]\n",
    "\n",
    "\n",
    "        train_acc   = num_train_correct / num_train_examples\n",
    "        train_loss  = train_loss / len(train_loader.dataset)\n",
    "\n",
    "\n",
    "        # --- EVALUATE ON VALIDATION SET -------------------------------------\n",
    "        model.eval()\n",
    "        val_loss       = 0.0\n",
    "        num_val_correct  = 0\n",
    "        num_val_examples = 0\n",
    "\n",
    "        for batch in test_loader:\n",
    "\n",
    "            x    = batch[0].to(device)\n",
    "            y    = batch[1].to(device)\n",
    "            yhat = model(x.float())\n",
    "            loss = loss_fn(yhat, y)\n",
    "\n",
    "            val_loss         += loss.data.item() * x.size(0)\n",
    "            num_val_correct  += (torch.max(yhat, 1)[1] == y).sum().item()\n",
    "            num_val_examples += y.shape[0]\n",
    "            \n",
    "            _, preds = torch.max(yhat, 1)\n",
    "            for t, p in zip(y.view(-1), preds.view(-1)):\n",
    "                confusion_matrixK[t.long(), p.long()] += 1\n",
    "            \n",
    "        val_acc  = num_val_correct / num_val_examples\n",
    "        val_loss = val_loss / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "        if epoch == 1 or epoch % 2 == 0:\n",
    "            print('Epoch %3d/%3d, train loss: %5.2f, train acc: %5.2f, val loss: %5.2f, val acc: %5.2f' % \\\n",
    "                (epoch, epochs, train_loss, train_acc, val_loss, val_acc))\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "    # END OF TRAINING LOOP\n",
    "\n",
    "\n",
    "    end_time_sec       = time.time()\n",
    "    total_time_sec     = end_time_sec - start_time_sec\n",
    "    time_per_epoch_sec = total_time_sec / epochs\n",
    "    print()\n",
    "    print('Time total:     %5.2f sec' % (total_time_sec))\n",
    "    print('Time per epoch: %5.2f sec' % (time_per_epoch_sec))\n",
    "    print(confusion_matrixK)\n",
    "    print(\"Confusion matrix per-class accuracy\")\n",
    "    print(100*confusion_matrixK.diag()/confusion_matrixK.sum(1))\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print(\"Metrics\")\n",
    "    #plt.figure(figsize=(15,10))\n",
    "    y_test=y\n",
    "    y_pred=np.argmax(yhat.detach(),axis=1)\n",
    "    confusion = confusion_matrix(y_test.detach(), y_pred.detach())\n",
    "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "    print('\\nAccuracy: {:.2f}\\n'.format(accuracy_score(y_test, y_pred)))\n",
    "\n",
    "    print('Micro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "    print('Micro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "    print('Micro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='micro')))\n",
    "\n",
    "    print('Macro Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='macro')))\n",
    "    print('Macro Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='macro')))\n",
    "    print('Macro F1-score: {:.2f}\\n'.format(f1_score(y_test, y_pred, average='macro')))\n",
    "\n",
    "    print('Weighted Precision: {:.2f}'.format(precision_score(y_test, y_pred, average='weighted')))\n",
    "    print('Weighted Recall: {:.2f}'.format(recall_score(y_test, y_pred, average='weighted')))\n",
    "    print('Weighted F1-score: {:.2f}'.format(f1_score(y_test, y_pred, average='weighted')))\n",
    "\n",
    "    from sklearn.metrics import classification_report\n",
    "    print('\\nClassification Report\\n')\n",
    "    print(classification_report(y_test, y_pred, target_names=['Poacher with gun', 'Poacher with arrow', 'No Poacher']))\n",
    "#     class_names = list(label2class.values())\n",
    "#     df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names).astype(int)\n",
    "#     heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "\n",
    "#     heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right',fontsize=15)\n",
    "#     heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right',fontsize=15)\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65255130",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train() called: model=SimpleNet, opt=Adam(lr=0.001000), epochs=20, device=cpu\n",
      "\n",
      "Epoch   1/ 20, train loss:  1.56, train acc:  0.48, val loss:  0.30, val acc:  0.55\n",
      "Epoch   2/ 20, train loss:  0.50, train acc:  0.65, val loss:  0.21, val acc:  0.67\n",
      "Epoch   4/ 20, train loss:  0.38, train acc:  0.76, val loss:  0.20, val acc:  0.68\n",
      "Epoch   6/ 20, train loss:  0.29, train acc:  0.81, val loss:  0.14, val acc:  0.78\n",
      "Epoch   8/ 20, train loss:  0.20, train acc:  0.89, val loss:  0.13, val acc:  0.82\n",
      "Epoch  10/ 20, train loss:  0.17, train acc:  0.90, val loss:  0.15, val acc:  0.80\n",
      "Epoch  12/ 20, train loss:  0.09, train acc:  0.95, val loss:  0.16, val acc:  0.82\n",
      "Epoch  14/ 20, train loss:  0.06, train acc:  0.97, val loss:  0.15, val acc:  0.85\n",
      "Epoch  16/ 20, train loss:  0.09, train acc:  0.95, val loss:  0.29, val acc:  0.78\n",
      "Epoch  18/ 20, train loss:  0.09, train acc:  0.95, val loss:  0.25, val acc:  0.81\n",
      "Epoch  20/ 20, train loss:  0.09, train acc:  0.96, val loss:  0.20, val acc:  0.84\n",
      "\n",
      "Time total:     7346.09 sec\n",
      "Time per epoch: 367.30 sec\n",
      "tensor([[273.,   4.,  16.],\n",
      "        [  7., 243.,  40.],\n",
      "        [ 19.,  56., 226.]])\n",
      "Confusion matrix per-class accuracy\n",
      "tensor([93.1741, 83.7931, 75.0831])\n",
      "Metrics\n",
      "\n",
      "Accuracy: 0.95\n",
      "\n",
      "Micro Precision: 0.95\n",
      "Micro Recall: 0.95\n",
      "Micro F1-score: 0.95\n",
      "\n",
      "Macro Precision: 0.94\n",
      "Macro Recall: 0.96\n",
      "Macro F1-score: 0.95\n",
      "\n",
      "Weighted Precision: 0.96\n",
      "Weighted Recall: 0.95\n",
      "Weighted F1-score: 0.95\n",
      "\n",
      "Classification Report\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "  Poacher with gun       1.00      0.89      0.94         9\n",
      "Poacher with arrow       1.00      1.00      1.00         6\n",
      "        No Poacher       0.83      1.00      0.91         5\n",
      "\n",
      "          accuracy                           0.95        20\n",
      "         macro avg       0.94      0.96      0.95        20\n",
      "      weighted avg       0.96      0.95      0.95        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = train(\n",
    "    model = model,\n",
    "    optimizer = optimizer,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    "    test_loader = test_loader,   \n",
    "    device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09be6593",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2893/2407323288.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "acc = history['acc']\n",
    "val_acc = history['val_acc']\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'b', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d5380ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accu=[]\n",
    "train_losses=[]\n",
    "def train(num_epochs):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('\\nEpoch : %d'%epoch)\n",
    "    \n",
    "        model.train()\n",
    "\n",
    "        running_loss=0\n",
    "        correct=0\n",
    "        total=0\n",
    "        #exp_lr_scheduler.step()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "    #         data,target=data[0].to(device),data[1].to(device)\n",
    "            data, target = Variable(data), Variable(target)\n",
    "\n",
    "    #         if torch.cuda.is_available():\n",
    "    #             data = data.cuda()\n",
    "    #             target = target.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            output = model(data)\n",
    "\n",
    "            loss = loss_fn(output, target)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "\n",
    "        train_loss=running_loss/len(train_loader)\n",
    "        accu=100.*correct/total\n",
    "\n",
    "        train_accu.append(accu)\n",
    "        train_losses.append(train_loss)\n",
    "        print('Train Loss: %.3f | Accuracy: %.3f'%(train_loss,accu))\n",
    "        if (batch_idx + 1)% 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, (batch_idx + 1) * len(data), len(train_loader.dataset),\n",
    "                100. * (batch_idx + 1) / len(train_loader), loss.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e719d0af",
   "metadata": {},
   "source": [
    "#Accuracy is the number of correct classifications / the total amount of classifications.\n",
    "#I am dividing it by the total number of the dataset because I have finished one epoch.\n",
    "#If you would like to calculate the loss for each epoch, divide \n",
    "the running_loss by the number of batches and append it to train_losses in each epoch.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef7ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b1651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
